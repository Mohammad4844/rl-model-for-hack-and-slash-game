{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yB8rGe5N_1I"
      },
      "outputs": [],
      "source": [
        "!pip install mlagents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o exec_Data.zip"
      ],
      "metadata": {
        "id": "rA6IF4z1PiIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIVqBzGbQBPL",
        "outputId": "de7524d5-ed96-434b-9d1a-328007313f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exec_Data  exec_Data.zip  exec.x86_64  __MACOSX  player.yaml  results  sample_data  UnityPlayer.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R 755 /content/exec.x86_64\n",
        "!chmod -R 755 /content/UnityPlayer.so"
      ],
      "metadata": {
        "id": "qLQKwswYQvha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mlagents-learn player.yaml --env=exec --run-id=Player1 --torch-device=cuda --num-envs=3 --no-graphics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_xD2asGPIB5",
        "outputId": "74b490ea-876f-4b52-f5f2-c5074956d013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "\n",
            "            ┐  ╖\n",
            "        ╓╖╬│╡  ││╬╖╖\n",
            "    ╓╖╬│││││┘  ╬│││││╬╖\n",
            " ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗\n",
            " ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣\n",
            " ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣\n",
            " ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣\n",
            " ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜\n",
            " ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣\n",
            " ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣\n",
            "   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣\n",
            "      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜\n",
            "          ╙╬╬╬╣╣╣╜\n",
            "             ╙\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 1.0.0,\n",
            "  ml-agents-envs: 1.0.0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 2.2.1+cu121\n",
            "[INFO] Connected to Unity environment with package version 3.0.0-exp.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 3.0.0-exp.1 and communication version 1.5.0\n",
            "[INFO] Connected to Unity environment with package version 3.0.0-exp.1 and communication version 1.5.0\n",
            "[INFO] Connected new brain: PlayerAgent?team=0\n",
            "[INFO] Connected new brain: PlayerAgent?team=0\n",
            "[INFO] Connected new brain: PlayerAgent?team=0\n",
            "[INFO] Hyperparameters for behavior name PlayerAgent: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t128\n",
            "\t  buffer_size:\t4096\n",
            "\t  learning_rate:\t0.0002\n",
            "\t  beta:\t0.001\n",
            "\t  epsilon:\t0.15\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tconstant\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tTrue\n",
            "\t  hidden_units:\t128\n",
            "\t  num_layers:\t3\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\t  curiosity:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t0.1\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\t    learning_rate:\t0.0003\n",
            "\t    encoding_size:\tNone\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\teven_checkpoints:\tFalse\n",
            "\tmax_steps:\t600000\n",
            "\ttime_horizon:\t1000\n",
            "\tsummary_freq:\t5000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\tNone\n",
            "\tbehavioral_cloning:\tNone\n",
            "/usr/local/lib/python3.10/dist-packages/mlagents/trainers/torch_entities/utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  torch.nn.functional.one_hot(_act.T, action_size[i]).float()\n",
            "[INFO] PlayerAgent. Step: 5000. Time Elapsed: 41.898 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 10000. Time Elapsed: 80.020 s. Mean Reward: 0.194. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 15000. Time Elapsed: 107.560 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 20000. Time Elapsed: 145.260 s. Mean Reward: 0.049. Std of Reward: 0.449. Training.\n",
            "[INFO] PlayerAgent. Step: 25000. Time Elapsed: 179.999 s. Mean Reward: -0.207. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 30000. Time Elapsed: 209.110 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 35000. Time Elapsed: 236.354 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 40000. Time Elapsed: 274.472 s. Mean Reward: 0.621. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 45000. Time Elapsed: 302.276 s. Mean Reward: 0.676. Std of Reward: 0.107. Training.\n",
            "[INFO] PlayerAgent. Step: 50000. Time Elapsed: 341.075 s. Mean Reward: 0.573. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 55000. Time Elapsed: 369.101 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 60000. Time Elapsed: 398.342 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 65000. Time Elapsed: 435.882 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 70000. Time Elapsed: 464.356 s. Mean Reward: 1.820. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 75000. Time Elapsed: 490.787 s. Mean Reward: 0.381. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 80000. Time Elapsed: 528.377 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 85000. Time Elapsed: 558.318 s. Mean Reward: 0.339. Std of Reward: 0.285. Training.\n",
            "[INFO] PlayerAgent. Step: 90000. Time Elapsed: 587.953 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 95000. Time Elapsed: 618.974 s. Mean Reward: 0.517. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 100000. Time Elapsed: 657.431 s. Mean Reward: -0.425. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 105000. Time Elapsed: 685.440 s. Mean Reward: -0.103. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 110000. Time Elapsed: 718.892 s. Mean Reward: 0.677. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 115000. Time Elapsed: 746.902 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 120000. Time Elapsed: 779.055 s. Mean Reward: 0.417. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 125000. Time Elapsed: 814.673 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 130000. Time Elapsed: 843.070 s. Mean Reward: -0.195. Std of Reward: 0.178. Training.\n",
            "[INFO] PlayerAgent. Step: 135000. Time Elapsed: 869.948 s. Mean Reward: 0.758. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 140000. Time Elapsed: 908.148 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 145000. Time Elapsed: 945.320 s. Mean Reward: 0.759. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 150000. Time Elapsed: 975.023 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 155000. Time Elapsed: 1010.909 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 160000. Time Elapsed: 1040.407 s. Mean Reward: 0.436. Std of Reward: 0.786. Training.\n",
            "[INFO] PlayerAgent. Step: 165000. Time Elapsed: 1072.697 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 170000. Time Elapsed: 1102.607 s. Mean Reward: 0.181. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 175000. Time Elapsed: 1137.377 s. Mean Reward: -0.836. Std of Reward: 0.149. Training.\n",
            "[INFO] PlayerAgent. Step: 180000. Time Elapsed: 1162.363 s. Mean Reward: -0.254. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 185000. Time Elapsed: 1196.047 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 190000. Time Elapsed: 1224.824 s. Mean Reward: 0.192. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 195000. Time Elapsed: 1256.730 s. Mean Reward: -0.312. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 200000. Time Elapsed: 1287.571 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 205000. Time Elapsed: 1321.494 s. Mean Reward: -0.374. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 210000. Time Elapsed: 1355.703 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 215000. Time Elapsed: 1382.680 s. Mean Reward: -0.259. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 220000. Time Elapsed: 1412.573 s. Mean Reward: 0.990. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 225000. Time Elapsed: 1447.381 s. Mean Reward: 1.277. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 230000. Time Elapsed: 1479.195 s. Mean Reward: 1.022. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 235000. Time Elapsed: 1516.391 s. Mean Reward: -0.718. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 240000. Time Elapsed: 1542.529 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 245000. Time Elapsed: 1569.585 s. Mean Reward: 0.091. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 250000. Time Elapsed: 1612.988 s. Mean Reward: -0.140. Std of Reward: 0.865. Training.\n",
            "[INFO] PlayerAgent. Step: 255000. Time Elapsed: 1640.146 s. Mean Reward: 0.631. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 260000. Time Elapsed: 1672.753 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 265000. Time Elapsed: 1706.871 s. Mean Reward: 0.101. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 270000. Time Elapsed: 1735.463 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 275000. Time Elapsed: 1771.575 s. Mean Reward: -0.709. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 280000. Time Elapsed: 1801.464 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 285000. Time Elapsed: 1832.996 s. Mean Reward: 1.435. Std of Reward: 0.415. Training.\n",
            "[INFO] PlayerAgent. Step: 290000. Time Elapsed: 1869.468 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 295000. Time Elapsed: 1897.324 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 300000. Time Elapsed: 1931.983 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 305000. Time Elapsed: 1965.435 s. Mean Reward: 0.787. Std of Reward: 0.260. Training.\n",
            "[INFO] PlayerAgent. Step: 310000. Time Elapsed: 1993.620 s. Mean Reward: -0.300. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 315000. Time Elapsed: 2026.946 s. Mean Reward: -0.086. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 320000. Time Elapsed: 2061.404 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 325000. Time Elapsed: 2087.768 s. Mean Reward: -0.581. Std of Reward: 0.149. Training.\n",
            "[INFO] PlayerAgent. Step: 330000. Time Elapsed: 2121.926 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 335000. Time Elapsed: 2150.980 s. Mean Reward: -0.494. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 340000. Time Elapsed: 2182.690 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 345000. Time Elapsed: 2216.510 s. Mean Reward: 0.088. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 350000. Time Elapsed: 2251.608 s. Mean Reward: -0.176. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 355000. Time Elapsed: 2283.059 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 360000. Time Elapsed: 2317.256 s. Mean Reward: 0.910. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 365000. Time Elapsed: 2347.962 s. Mean Reward: 0.489. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 370000. Time Elapsed: 2378.575 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 375000. Time Elapsed: 2409.261 s. Mean Reward: -0.711. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 380000. Time Elapsed: 2441.363 s. Mean Reward: 0.559. Std of Reward: 0.243. Training.\n",
            "[INFO] PlayerAgent. Step: 385000. Time Elapsed: 2473.647 s. Mean Reward: -0.382. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 390000. Time Elapsed: 2503.251 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 395000. Time Elapsed: 2535.818 s. Mean Reward: -0.845. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 400000. Time Elapsed: 2564.399 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 405000. Time Elapsed: 2600.432 s. Mean Reward: 0.321. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 410000. Time Elapsed: 2629.000 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 415000. Time Elapsed: 2659.270 s. Mean Reward: 0.614. Std of Reward: 0.796. Training.\n",
            "[INFO] PlayerAgent. Step: 420000. Time Elapsed: 2688.803 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 425000. Time Elapsed: 2722.787 s. Mean Reward: -0.755. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 430000. Time Elapsed: 2754.167 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 435000. Time Elapsed: 2784.944 s. Mean Reward: 0.050. Std of Reward: 0.491. Training.\n",
            "[INFO] PlayerAgent. Step: 440000. Time Elapsed: 2818.996 s. Mean Reward: 0.454. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 445000. Time Elapsed: 2853.761 s. Mean Reward: -0.664. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 450000. Time Elapsed: 2885.317 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 455000. Time Elapsed: 2911.991 s. Mean Reward: -0.431. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 460000. Time Elapsed: 2944.875 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 465000. Time Elapsed: 2981.340 s. Mean Reward: -0.453. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 470000. Time Elapsed: 3007.658 s. Mean Reward: 0.326. Std of Reward: 0.597. Training.\n",
            "[INFO] PlayerAgent. Step: 475000. Time Elapsed: 3044.623 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 480000. Time Elapsed: 3072.094 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 485000. Time Elapsed: 3103.663 s. Mean Reward: -0.135. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 490000. Time Elapsed: 3136.961 s. Mean Reward: -0.039. Std of Reward: 0.097. Training.\n",
            "[INFO] PlayerAgent. Step: 495000. Time Elapsed: 3165.249 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 500000. Time Elapsed: 3202.806 s. Mean Reward: 0.325. Std of Reward: 0.000. Training.\n",
            "[INFO] Exported results/Player1/PlayerAgent/PlayerAgent-499736.onnx\n",
            "[INFO] PlayerAgent. Step: 505000. Time Elapsed: 3231.954 s. Mean Reward: -0.731. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 510000. Time Elapsed: 3257.951 s. Mean Reward: -0.284. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 515000. Time Elapsed: 3292.721 s. Mean Reward: -0.334. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 520000. Time Elapsed: 3322.631 s. Mean Reward: -0.056. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 525000. Time Elapsed: 3358.541 s. Mean Reward: -0.410. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 530000. Time Elapsed: 3393.285 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 535000. Time Elapsed: 3423.686 s. Mean Reward: -0.092. Std of Reward: 0.090. Training.\n",
            "[INFO] PlayerAgent. Step: 540000. Time Elapsed: 3454.539 s. Mean Reward: 0.150. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 545000. Time Elapsed: 3481.079 s. Mean Reward: -1.120. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 550000. Time Elapsed: 3515.653 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 555000. Time Elapsed: 3551.430 s. Mean Reward: -0.124. Std of Reward: 0.231. Training.\n",
            "[INFO] PlayerAgent. Step: 560000. Time Elapsed: 3576.853 s. Mean Reward: -0.639. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 565000. Time Elapsed: 3613.004 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 570000. Time Elapsed: 3640.965 s. Mean Reward: -0.545. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 575000. Time Elapsed: 3675.453 s. Mean Reward: 0.756. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 580000. Time Elapsed: 3703.961 s. Mean Reward: 1.110. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 585000. Time Elapsed: 3735.960 s. No episode was completed since last summary. Training.\n",
            "[INFO] PlayerAgent. Step: 590000. Time Elapsed: 3772.177 s. Mean Reward: 0.301. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 595000. Time Elapsed: 3798.131 s. Mean Reward: 0.356. Std of Reward: 0.000. Training.\n",
            "[INFO] PlayerAgent. Step: 600000. Time Elapsed: 3830.139 s. No episode was completed since last summary. Training.\n",
            "[INFO] Exported results/Player1/PlayerAgent/PlayerAgent-600214.onnx\n",
            "[INFO] Copied results/Player1/PlayerAgent/PlayerAgent-600214.onnx to results/Player1/PlayerAgent.onnx.\n"
          ]
        }
      ]
    }
  ]
}